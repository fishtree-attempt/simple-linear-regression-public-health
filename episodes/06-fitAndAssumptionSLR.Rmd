---
source: Rmd
title: "Assessing simple linear regression model fit and assumptions"
objectives:
  - "Understand what is meant by model fit."
  - "Use the R squared value as a measure of model fit."
  - "Describe the assumptions of the simple linear regression model."
  - "Assess whether the assumptions of the simple linear regression model have been violated."
  - "Become aware of alternative regression method that can be used when the simple linear regression assumptions are violated."
keypoints:
  - "DEF"
questions:
  - "GHI"
teaching: 10
execises: 10
---

```{r, include=FALSE}
source("../bin/chunk-options.R")
source("../bin/obtain_data.R")
knitr_fig_path("06-")
library(ggplot2)
library(dplyr)
library(jtools)
library(cowplot)
```

```{r models from previous episodes, echo = FALSE}
Weight_Height_lm <- dat %>%
  filter(Age > 17) %>%
  lm(formula = Weight ~ Height)

TotChol_SmokeNow_lm <- lm(formula = TotChol ~ SmokeNow, data = dat)
```


>## Exercise
>Find the R-squared value for the `summ` output of our `TotChol_BMI_lm` model from 
>[episode 2](https://carpentries-incubator.github.io/simple-linear-regression-public-health/02-singleContPred).
>What proportion of variation in Total Cholesterol is explained by BMI in our model?  
{: .challenge}

```{r PhysActive vs Pulse model}
TotChol_BMI_lm <- lm(formula = TotChol ~ BMI, data = dat)

summ(TotChol_BMI_lm, confint = TRUE, digits = 3)
```



Assumptions of the simple linear regression model:  
1. **Validity**: the model is appropriate for the research question. This sounds obvious, but it is easy to come to unreliable conclusions because of inappropriate model choice. Validity is assessed in three ways:  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;A) Does the outcome variable reflect the *phenomenon of interest*? For example, it would not be appropriate to take our `Pulse` vs `PhysActive` model as representative of the effect of physical activity on general health.  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;B) Does the model include *all relevant explanatory variables*? For example, we might decide that our model of `TotChol` vs `BMI` requires inclusion of the `SmokeNow` variable.  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;C) Does the model generalise to our *case of interest*? For example, it would not be appropriate to model the effect of a *change* in physical activity on pulse using out `Pulse` vs `PhysActive` model. Neither would it be appropriate to take the model, which was constructed using people of all ages, as representative of the effect of physical activity on pulse in those aged 70+. In both examples, we would need a model constructed on different data to answer the question.  

>## Exercise
You are   
{: .challenge}


2. **Representativeness**: the *sample* is representative of the *population*. More specifically, the individuals from which our sample is formed are representative of the population of interest. The exception to this requirement is that the sample distribution can differ from the population distribution in the explanatory variables included in the model. For example, let us assume that in the American population, 40% of individuals are physically active. In the NHANES data, ~56% of individuals are physically active. This discrepancy is dealt with by our `Pulse` vs `PhysActive` model, since `PhysActive` is an explanatory variable. However, if the majority of individuals in the NHANES data were over the age of 70, then our `Pulse` vs `PhysActive` model would not be representative of the American population. We would need to include `Age` as an explanatory variable to meet the representativeness assumption.  
3. **Linearity and additivity**: our outcome variable has a linear, additive relationship with the explanatory variables.  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The *linearity* component means that each explanatory variable needs to be modeled through a linear relationship with the outcome variable. For example, see plot **A** below. The relationship between `BPDiaAve` and `AgeMonths` is non-linear and our model `lm(formula = BPDiaAve ~ AgeMonths , data=dat)` fails to capture this non-linear relationship. Adding a squared term to our model, designated by `I(AgeMonths^2)`, allows our model to capture the non-linear relationship (see plot **B**). Thus, the model `lm(formula = BPDiaAve ~ AgeMonths + I(AgeMonths^2), data=dat)` does not violate the linearity assumption.  

```{r non-linearity example}
BPDiaAve_AgeMonths_lm <- lm(formula = BPDiaAve ~ AgeMonths , data = dat)

p1 <- effect_plot(BPDiaAve_AgeMonths_lm, pred = AgeMonths, 
                  plot.points = TRUE, interval = TRUE,
                  colors = c("red")) +
  ylab("Combined diastolic blood pressure") +
  xlab("Age in Months") +
  ggtitle("Not a linear relationship") +
  theme_bw()

BPDiaAve_AgeMonthsSQ_lm <- lm(formula = BPDiaAve ~ AgeMonths + I(AgeMonths^2), data=dat)

p2 <- effect_plot(BPDiaAve_AgeMonthsSQ_lm, pred = AgeMonths, 
                  plot.points = TRUE, interval = TRUE,
                  colors = c("red")) +
  ylab("Combined diastolic blood pressure") +
  xlab("Age in Months") +
  ggtitle("Non-linear relationship modelled \nusing an appropriate \nsimple linear regression model") +
  theme_bw()

plot_grid(p1, p2, labels = c("A)", "B)"))


```

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The *additivity* component means that the effect of any explanatory variable on the outcome variable does not depend on another explanatory variable in the model. When this assumption is violated, it can be mitigated by including an interaction term in the model. We will cover interaction terms in the [multiple linear regression for public health lesson](https://carpentries-incubator.github.io/multiple-linear-regression-public-health/).  
4. **Independent errors**: the residuals must be independent of one another. This assumption is violated when observations are not a random sample of the population, i.e. when observations are non-independent. For example, if we measure individual's weights four times over the course of a year, then our data will contain four non-independent observations per individual. As a result, the residuals will also not be independent. This can be overcome using random effects, which we will cover in the [linear mixed effects models for public health lesson] (https://carpentries-incubator.github.io/linear-mixed-models-public-health/).  

>## Exercise
> In which of the following scenarios would the independent errors assumption likely be violated?
> 
> A) We are modeling the effect of a fitness program on people's fitness level. Our data consists of weekly fitness measurements on the same group of individuals.   
> B) We are modeling the effect of dementia prevention treatments on life expectancy, with each participant  coming from one of five care homes. Our data consists of life expectancy, whether an individual was on a dementia prevention treatment and the care home that the individual was in.  
> 
> C) We are modeling the effect of income on home size in a random sample of the adult UK population.
> 
> > ## Solution
> > A) Since we have multiple observations per participant, our observations are not independent. We would hereby violate the independent errors assumption.  
> > B) Our observations are non-independent because multiple individuals will have belonged to the same carehome. In this case, adding carehome to our model would allow us to overcome the violation of the independent errors assumption.  
> > C) Since our data is a random sample, we are not violating the independent errors assumption through non-independence in our data. 
> {: .solution}
{: .challenge}

5. **Equal variance of errors (heteroscedasticity)**: the magnitude of variation in the residuals is not different across an explanatory variable. Violation of this assumption can result in unreliable estimates of the standard errors of coefficients, which may impact statistical inference. Predictions from the model may become unreliable too. Transformation can sometimes be used to resolve heteroscedasticity. In other cases, weighted least squares can be used (not discussed here).  

```{r}
test <- lm(formula = SexNumPartYear ~ SexOrientation, data = dat)

effect_plot(test, pred = SexOrientation, interval = TRUE, partial.residuals = TRUE, jitter = c(0.2, 0))

summ(test)

#predict(test)
```


6. **Normality of errors**: the errors follow a Normal distribution. When this assumption is strongly violated, predictions from the model are less reliable. Small deviations from normality may pose less of an issue. One way to check this assumption is to plot a histogram of the residuals and to ask whether it looks strongly non-normal (e.g. bimodal or uniform).

For example, looking at a histogram of the residuals of out `Height_Weight_lm` model reveals a distribution that is slightly skewed. Since this is not a strong deviation from normality, we do not have to worry about violating the assumption. 

```{r check normality example, message = FALSE}
residuals <- tibble(resid = resid(Weight_Height_lm))

ggplot(residuals, aes(x=resid)) +
  geom_histogram() +
  ylab("Count") +
  xlab("Residual")
```

>## Exercise
> Construct a histogram of the residuals of the `TotChol_SmokeNow_lm` model. Does the distribution suggest that the normality assumption is violated?
> > ## Solution
> > ```{r check normality challenge, message = FALSE}
> > residuals <- tibble(resid = resid(TotChol_SmokeNow_lm))
> > 
> > ggplot(residuals, aes(x=resid)) +
> >   geom_histogram() +
> >   ylab("Count") +
> >   xlab("Residual")
> > ```  
> > 
> > Since the distribution is only slightly skewed, we do not have to worry about violating the normality assumption. 
> {: .solution}
{: .challenge}
