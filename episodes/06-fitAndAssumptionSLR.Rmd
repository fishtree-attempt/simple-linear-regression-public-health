---
source: Rmd
title: "Assessing simple linear regression model fit and assumptions"
objectives:
  - "Understand what is meant by model fit."
  - "Use the R squared value as a measure of model fit."
  - "Describe the assumptions of the simple linear regression model."
  - "Assess whether the assumptions of the simple linear regression model have been violated."
  - "Become aware of alternative regression method that can be used when the simple linear regression assumptions are violated."
keypoints:
  - "DEF"
questions:
  - "GHI"
teaching: 10
execises: 10
---

```{r, include=FALSE}
source("../bin/chunk-options.R")
source("../bin/obtain_data.R")
knitr_fig_path("06-")
library(ggplot2)
library(dplyr)
library(jtools)
library(patchwork)
library(tidyr)
```

```{r models from previous episodes, echo = FALSE}
Weight_Height_lm <- dat %>%
  filter(Age > 17) %>%
  lm(formula = Weight ~ Height)

TotChol_SmokeNow_lm <- lm(formula = TotChol ~ SmokeNow, data = dat)

UrineFlow_UrineVol_lm <- lm(formula = UrineFlow1 ~ UrineVol1, data = dat)
test_lm <- dat %>%
  drop_na(UrineFlow1) %>%
  mutate(UrineFlow1_log = log(UrineFlow1 + 0.0001) + 0.0001) %>%
  lm(formula = UrineFlow1_log ~ UrineVol1)

test_lm2 <- lm(formula = UrineFlow1 ~ UrineVol1 + I(UrineVol1^2), data = dat)
```


>## Exercise
>Find the R-squared value for the `summ` output of our `TotChol_BMI_lm` model from 
>[episode 2](https://carpentries-incubator.github.io/simple-linear-regression-public-health/02-singleContPred).
>What proportion of variation in Total Cholesterol is explained by BMI in our model?  
{: .challenge}

```{r PhysActive vs Pulse model}
TotChol_BMI_lm <- lm(formula = TotChol ~ BMI, data = dat)

summ(TotChol_BMI_lm, confint = TRUE, digits = 3)
```



Assumptions of the simple linear regression model:  
1. **Validity**: the model is appropriate for the research question. This sounds obvious, but it is easy to come to unreliable conclusions because of inappropriate model choice. Validity is assessed in three ways:  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;A) Does the outcome variable reflect the *phenomenon of interest*? For example, it would not be appropriate to take our `Pulse` vs `PhysActive` model as representative of the effect of physical activity on general health.  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;B) Does the model include *all relevant explanatory variables*? For example, we might decide that our model of `TotChol` vs `BMI` requires inclusion of the `SmokeNow` variable.  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;C) Does the model generalise to our *case of interest*? For example, it would not be appropriate to model the effect of a *change* in physical activity on pulse using out `Pulse` vs `PhysActive` model. Neither would it be appropriate to take the model, which was constructed using people of all ages, as representative of the effect of physical activity on pulse in those aged 70+. In both examples, we would need a model constructed on different data to answer the question.  

>## Exercise
You are   
{: .challenge}


2. **Representativeness**: the *sample* is representative of the *population*. More specifically, the individuals from which our sample is formed are representative of the population of interest. The exception to this requirement is that the sample distribution can differ from the population distribution in the explanatory variables included in the model. For example, let us assume that in the American population, 40% of individuals are physically active. In the NHANES data, ~56% of individuals are physically active. This discrepancy is dealt with by our `Pulse` vs `PhysActive` model, since `PhysActive` is an explanatory variable. However, if the majority of individuals in the NHANES data were over the age of 70, then our `Pulse` vs `PhysActive` model would not be representative of the American population. We would need to include `Age` as an explanatory variable to meet the representativeness assumption.  
3. **Linearity and additivity**: our outcome variable has a linear, additive relationship with the explanatory variables.  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The *linearity* component means that each explanatory variable needs to be modeled through a linear relationship with the outcome variable. We learned to check for this relationship before fitting our model, through the xploratory plots at the start of the previous episodes. For an example where the linearity assumption is violated, see the left plot below. The relationship between `BPDiaAve` and `AgeMonths` is non-linear and our model `lm(formula = BPDiaAve ~ AgeMonths , data=dat)` fails to capture this non-linear relationship. Adding a squared term to our model, designated by `I(AgeMonths^2)`, allows our model to capture the non-linear relationship (see the right     plot). Thus, the model `lm(formula = BPDiaAve ~ AgeMonths + I(AgeMonths^2), data=dat)` does not violate the linearity assumption.  

```{r non-linearity example}
BPDiaAve_AgeMonths_lm <- lm(formula = BPDiaAve ~ AgeMonths , data = dat)

p1 <- effect_plot(BPDiaAve_AgeMonths_lm, pred = AgeMonths, 
                  plot.points = TRUE, interval = TRUE,
                  colors = c("red")) +
  ylab("Combined diastolic blood pressure") +
  xlab("Age in Months") +
  ggtitle("Not a linear relationship") +
  theme_bw()

BPDiaAve_AgeMonthsSQ_lm <- lm(formula = BPDiaAve ~ AgeMonths + I(AgeMonths^2), data=dat)

p2 <- effect_plot(BPDiaAve_AgeMonthsSQ_lm, pred = AgeMonths, 
                  plot.points = TRUE, interval = TRUE,
                  colors = c("red")) +
  ylab("Combined diastolic blood pressure") +
  xlab("Age in Months") +
  ggtitle("Non-linear relationship modelled \nusing an appropriate \nsimple linear regression model") +
  theme_bw()

p1 + p2


```


>## Exercise
> In the example above we saw that squaring an explanatory variable can correct
> for curvature seen in the outcome variable along the explanatory variable. 
>
> In the following example, we will see that taking the log transformation 
> of the dependent variable can also sometimes be an effective solution
> to non-linearity.
> 
> Firstly, fit a linear regression model of insulin (`Insulin`) as a function
> of age in months (`AgeMonths`). Create an effect plot using `jtools`, ensuring
> that `point.alpha` is set to `0.2`. 
> > ## Solution
> > ```{r non-linearity challenge part 1, warning = FALSE}
> > Insulin_AgeMonths_lm <- lm(formula = Insulin ~ AgeMonths, data = dat)
> > 
> > effect_plot(Insulin_AgeMonths_lm, pred = AgeMonths, 
> >             plot.points = TRUE, interval = TRUE,
> >             colors = c("red"), 
> >             point.alpha = 0.2) +
> >   ylab("Insulin") +
> >   xlab("Age in Months")
> > ```  
> {: .solution}
> 
> Since the majority of points appear to lie in a slight  curve below the regression
> line, the lineasrity assumption appears to have been violated. 
> 
> We will explore the log transformation as a potential solution. Fit a linear
> regression model as before, however change `Insulin` in the `lm()` command
> to `log(Insulin)`. Then create an effect plot using `jtools`, ensuring that 
> `point.alpha` is set to `0.2`. Is the relationship between `log(Insulin)` and 
> `AgeMonths` different from the relationship between `Insulin` and `AgeMonths`?
> > ## Solution
> > ```{r non-linearity challenge part 2, warning = FALSE}
> > LogInsulin_AgeMonths_lm <- lm(formula = log(Insulin) ~ AgeMonths , 
> >                               data = dat)
> > 
> > effect_plot(LogInsulin_AgeMonths_lm, pred = AgeMonths, 
> >             plot.points = TRUE, interval = TRUE,
> >             colors = c("red"), 
> >             point.alpha = 0.2) +
> >   ylab("Log(Insulin)") +
> >   xlab("Age in Months")
> > ```
> > 
> > The non-linear relationship has now been transformed into a linear 
> > relationship by taking the log transformation of the response. 
> {: .solution}
{: .challenge}

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The *additivity* component means that the effect of any explanatory variable on the outcome variable does not depend on another explanatory variable in the model. When this assumption is violated, it can be mitigated by including an interaction term in the model. We will cover interaction terms in the [multiple linear regression for public health lesson](https://carpentries-incubator.github.io/multiple-linear-regression-public-health/).  
4. **Independent errors**: the residuals must be independent of one another. This assumption is violated when observations are not a random sample of the population, i.e. when observations are non-independent. For example, if we measure individual's weights four times over the course of a year, then our data will contain four non-independent observations per individual. As a result, the residuals will also not be independent. This can be overcome using random effects, which we will cover in the [linear mixed effects models for public health lesson] (https://carpentries-incubator.github.io/linear-mixed-models-public-health/).  

>## Exercise
> In which of the following scenarios would the independent errors assumption likely be violated?
> 
> A) We are modeling the effect of a fitness program on people's fitness level. Our data consists of weekly fitness measurements on the same group of individuals.   
> B) We are modeling the effect of dementia prevention treatments on life expectancy, with each participant  coming from one of five care homes. Our data consists of life expectancy, whether an individual was on a dementia prevention treatment and the care home that the individual was in.  
> 
> C) We are modeling the effect of income on home size in a random sample of the adult UK population.
> 
> > ## Solution
> > A) Since we have multiple observations per participant, our observations are not independent. We would hereby violate the independent errors assumption.  
> > B) Our observations are non-independent because multiple individuals will have belonged to the same carehome. In this case, adding carehome to our model would allow us to overcome the violation of the independent errors assumption.  
> > C) Since our data is a random sample, we are not violating the independent errors assumption through non-independence in our data. 
> {: .solution}
{: .challenge}

5. **Equal variance of errors (heteroscedasticity)**: the magnitude of variation in the residuals is not different across the fitted values or any explanatory variable. Violation of this assumption can result in unreliable estimates of the standard errors of coefficients, which may impact statistical inference. Predictions from the model may become unreliable too. Transformation can sometimes be used to resolve heteroscedasticity. In other cases, weighted least squares can be used (not discussed in this lesson).  

For example, we can study the relationship between the residuals and the fitted values of our `Height_Weight_lm` model. We store the residuals, fitted values and explanatory variable in a tibble named `residualData`. The residuals are accessed using `resid()`, the fitted values are accessed using `fitted()` and the explanatory variable (`Height`) is accessed through the `Height` column of `Weight_Height_lm$Height`.

We create a residuals vs. fitted plot named `p1` and a residuals vs. explanatory variable plot named `p2`.These are brought together using `p1 + p2`, where the `+` relies on the `patchwork` package being loaded. 

```{r heteroscedasticity example}
residualData <- tibble(resid = resid(Weight_Height_lm),
                    fitted = fitted(Weight_Height_lm),
                    height = Weight_Height_lm$model$Height)

p1 <- ggplot(residualData, aes(x = fitted, y = resid)) +
  geom_point(alpha = 0.03) +
  geom_smooth() +
  ylab("Residual") +
  xlab("Fitted values")

p2 <- ggplot(residualData, aes(x = height, y = resid)) +
  geom_point(alpha = 0.03) +
  geom_smooth() +
  ylab("Residuals") +
  xlab("Height")

p1 + p2

```

Since there is no obvious pattern in the residuals along the fitted values or
the explanatory variable, there is no reason to suspect that the equal variance
assumption has been violated. 


>## Exercise
> Create diagnistic plots to check for heteroscedasticity in our `UrineFlow_UrineVol_lm` model. Do you believe the equal variance assumption has been violated?
>
> > ## Solution
> > ```{r heteroscedasticity challenge, warning = FALSE}
> > residualData <- tibble(resid = resid(UrineFlow_UrineVol_lm),
> >                     fitted = fitted(UrineFlow_UrineVol_lm),
> >                     urinevol = UrineFlow_UrineVol_lm$model$UrineVol1)
> > 
> > p1 <- ggplot(residualData, aes(x = fitted, y = resid)) +
> >   geom_point(alpha = 0.03) +
> >   geom_smooth() +
> >   ylab("Residual") +
> >   xlab("Fitted values") +
> >   ylim(-5,10)
> > 
> > p2 <- ggplot(residualData, aes(x = urinevol, y = resid)) +
> >   geom_point(alpha = 0.03) +
> >   geom_smooth() +
> >   ylab("Residuals") +
> >   xlab("Urine Volume") +
> >   ylim(-5,10)
> > 
> > p1 + p2
> > ```  
> > 
> > Since the variation in the residuals appears to increase with an increase in fitted values and urine volume, the equal variance assumption appears to have been violated. 
> {: .solution}
{: .challenge}

6. **Normality of errors**: the errors follow a Normal distribution. When this assumption is strongly violated, predictions from the model are less reliable. Small deviations from normality may pose less of an issue. One way to check this assumption is to plot a histogram of the residuals and to ask whether it looks strongly non-normal (e.g. bimodal or uniform).

For example, looking at a histogram of the residuals of out `Height_Weight_lm` model reveals a distribution that is slightly skewed. Since this is not a strong deviation from normality, we do not have to worry about violating the assumption. 

```{r check normality example, message = FALSE}
residuals <- tibble(resid = resid(Weight_Height_lm))

ggplot(residuals, aes(x=resid)) +
  geom_histogram() +
  ylab("Count") +
  xlab("Residual")
```

>## Exercise
> Construct a histogram of the residuals of the `TotChol_SmokeNow_lm` model. Does the distribution suggest that the normality assumption is violated?
> > ## Solution
> > ```{r check normality challenge, message = FALSE}
> > residuals <- tibble(resid = resid(TotChol_SmokeNow_lm))
> > 
> > ggplot(residuals, aes(x=resid)) +
> >   geom_histogram() +
> >   ylab("Count") +
> >   xlab("Residual")
> > ```  
> > 
> > Since the distribution is only slightly skewed, we do not have to worry about violating the normality assumption. 
> {: .solution}
{: .challenge}
